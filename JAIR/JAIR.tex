%
% File emnlp2016.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{JAIR}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{todo}

% Uncomment this line for the final submission:
\emnlpfinalcopy

%  Enter the EMNLP Paper ID here:
%\def\emnlppaperid{***}

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Natural Language Processing in the Age of Deep Learning: A Review}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
% \author{Felix Hill \and Daniele Pighin\\
%  {\tt publication@emnlp2016.net}}

\date{}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}


\section{Introduction}
what this paper is about

\section{Backgrounnd}

History of neural language models

\newcite{bengio2003neural}
\newcite{miikkulainen1991natural}
\cite{miikkulainen1993subsymbolic}

\section{Language Modelling}

\section{Machine Translation}

\section{Machine Comprehension}

Following the development of recurrent neural networks capable of mapping from sequences to sequences and their successful application to machine translation, NLP researchers noticed that a similar approach could be effective in \emph{machine comprehension} style applications. The terms \emph{machine comprehension}, \emph{machine reading} typically refer to the broad class of task in which an algorithm has access to a passages of multiple sentences of prose (henceforth the \emph{context}) and a set of questions about the context of the passage (referred to here as \emph{queries}). The algorithm must select a correct (single or multi-word) answer to each question based on the information in the corresponding passage.  Machine comprehension have much in common with \emph{reading comprehension} questions that are used test human reasoning and/or linguistic proficiency in language learners. 

\subsection{Datasets and sub-tasks}
\paragraph{Deepmind News datasets} The current wave of neural network-based machine comprehension research was in large part triggered by the news comprehension dataset of~\newcite{hermann2015teaching}, compiled automatically from articles on the CNN and Daily Mail websites. News stories on both sites are summarised by a series of bullet points that summarise their main points. To develop their machine comprehension task,~\cite{hermann2015teaching} first identified all named entities in both the article and the bullet points using standard (NER) tools. A question in the CNN or Daily Mail dataset was then constructed by selecting an article, removing an entity from one of the bullet points in the summary and verifying that the entity does indeed appear in the article. To prevent the application of external knowledge to the task, the entities in the articles were replaced with symbolic identifiers. The task facing models is therefore to match the missing entity in each question to one of the symbolic identifiers identifiers in its article. The (smaller) CNN section of the Deepmind dataset alone consists of over 380,000 training and 4,000 test questions taked from over 90,000 distinct articles.

\paragraph{Facebook Children's Book Test (CBT)} The CBT is a multiple-choice test in which models must identify the missing word in a sentence from a children's book by using information from the previous 20 sentences in the book~\newcite{hill2015goldilocks}. The questions were selected such that both the missing word from the 21st sentence, and nine distractor candidate answers, appear at least once in the preceeding 20 sentences. The training and test questions are also (optionally) separated according to whether the missing word (and candidate choices) is a preposition, a verb, a common noun or a named entity. An important difference between the CBT and the Deepmind News dataset is that entities are not anonymised, so that general knowledge acquired from external resources can be used by models to improve performance on the test set. In total there are over 660,000 training and 10,000 test questions in the CBT, divided roughly equally according to the four question types. 

\paragraph{Stanford Question Answering (SQuAD) dataset} The SQuAD dataset,~\newcite{rajpurkar2016squad} crowdsourced English speakers to write questions that follow naturally from Wikipedia passages. Thus, models attempting the SQuAD task must answer authentic questions rather than cloze-style statements with missing words, bringing the task closer to genuine human reading comprehension. A further difference from the Deepmind and CBT datasets is that annotators were encouraged to write questions whose answers were multi-word segments in the article. For instance, the answer to the question \emph{what are two things that a computer always has} is the article segment \emph{a central processing unit (CPU) and some form of memory}. The SQuAD dataset consists of 90,000 training and 10,000 test questions. 

 \paragraph{TTI Who did What dataset} The Who did What dataset~\cite{onishi2016did} is based on articles in the English Gigaword Corpus. To compile the resource,~\newcite{onishi2016did} selected articles whose first sentence contained a person named entity. The person entity in question was removed from the sentence to form the question, the remainder of the article discarded, and a new (supporting) article retrieved that refers to the same person. To ensure they are sufficiently informative, the supporting articles were retrieved via an IE-style string matching procedure in which the question is used as the query. Questions were discarded if they could be answered by simple language model or frequency baselines, resulting in a bank of over 180,000 train and 10,000 test questions that humans can answer approximately 84\% of the time. As in the SQuAD dataset, answers in the Who did What dataset may consist of multiple words, although they are all person entities (\emph{the King of France}).

\paragraph{}Before the advent of neural machine comprehension models, research on machine comprehension typically relied on questions taken directly from real human reading comprehension questions~\cite{hirschman1999deep}. However, since such questions are usually produced by private education enterprises, large-scale datasets of this kind are not freely available for NLP research. Another notable forerunner to today's massive training and test resources is the \emph{MC Test}~\cite{richardson2013mctest}, which consists of 660 stories, and corresponding questions, all written by human annotators. The training data made available with the MC Test dataset is much smaller than more recent resources, and neural language models have not typically performed as well on this task as approaches that employ bespoke, hand-engineered symbolic features. However, recent work has shown that neural language models trained additionally on external data sources can perform competitively on the MC Test~\cite{trischler2016parallel}. Developing and understanding such transfer from general-purpose training data to specific small-scale applications is a key challenge for neural net approaches to machine comprehension and language understanding in general. 


\subsection{Models}
The first large-scale neural network approach to machine comprehension, the {\bf Deepmind recurrent readers}~\cite{hermann2015teaching}, exploits many of the techniques and principles of attention-based NMT (section XX). To answer a question from the Deepmind News dataset, the recurrent readers first apply a bidirectional RNN to the context article~\footnote{This encoding is similar to an attention-NMT model reading the source sentence, but in this case the bidirectional RNN covers multiple sentences}. This yields a variable-length distributed representation of the context containing a unique vector (the RNN hidden state) for each inter-word position in the article. 

In the \emph{attentive reader}, a single distributed representation of the query is computed using another bidirectional RNN, and used to compute attention weights over the set of context vectors. These are used to compute a single, fixed length (weighted sum) representation of the context, which is recombined with the query and fed to an output softmax optimised to predict the correct missing word from all possible words in the model's vocabulary.  In the \emph{impatient reader}, the attention weights, and hence a different weighted-sum representation of the article, is computed for each position in the bullet-point query. Thus, much like the attention-NMT model when processing a target sentence from the training set, the impatient reader can take a different perspective on the context depending on its current point of focus in the query. These perspectives are incremented to arrive at a (complex) single-vector representation of the article, based on which probabilities are computed for vocabulary words as potential answers (as in the attentive reader).

On the Deepmind News tasks, the recurrent reading models outperform a range of symbolic (non-neural) baselines, including those based on TFI-IDF and other techniques popular in information retrieval. They also outperform a neural baseline in which deep, bidirectional LSTM models read the query and the article in turn (as one long, conflated sequence) before predicting a possible answer.

This approach to large-scale machine comprehension was improved-upon by~\newcite{hill2015goldilocks} using a {\bf memory network}. Their model consists of four components. The first embeds the context in a finite set of distributed `memories'. The second weights these memories by matching them to a distributed representation of the query. The third computes a single representation of the memory as a weighted sum of the memories. The fourth predicts an output word via an output softmax over the model vocabulary. The network is trained end-to-end using a maximum likelihood objective. While a specific form of each of these components is also performed by the deepmind recurrent readers, the modular design of the memory network makes it possible to experiment with different strategies for each component.~\newcite{hill2015goldilocks} found particular methods for the first and second components that yielded improved performance on the Deepmind News dataset and strong benchmark performance on the CBT. The first component of the memory network that achieved this performance represented the context as a series of discrete 5-word windows.~\footnote{This representation probably allows the memory network to encode similar information to that encoded by a a bi-RNN at the point of reading the middle word in the window}. In the second component, it employed a `self-supervision' heuristic in which the weights in query and document representations were updated during training to directly increase the probability of retrieving the correct answer from the context relative to the other candidate answers. 

\newcite{kadlec2016text} continued the trend of improvement with the {\bf attention sum reader}, an elegant solution that combines effective aspects of both memory network and the Deepmind readers. Their approach is \emph{candidate-centric}: rather than estimating the probability of possible answers from a large vocabulary, it simply estimates the probability that some (or all) of the words or segments in the context is the correct answer. Consequently, it avoids the computation of an expensive and inefficient softmax over a large vocabulary, and can focus directly on locating a part of the context than indicates the correct answer. Specifically, the model reads the context using bidirectional LSTMs and stores representations focused on every candidate answer (the anonymous en will effectively preclude these simpler solutions andtities in the Deepmind News task or candidate answers in the CBT). A separate network represents the query (again with a bi-LSTM), and the candidates are scored based on their match with the query representation. The model is then simply trained to score correct answers above other candidates.  

The attention sum reader underlines the value of bidirectional recurrent nets in providing variable (`soft') windows of focus over unstructured text. Together with the self-supervision heuristic used by~\newcite{hill2015goldilocks} to train memory networks, it also highlights the importance (and the challenge) of learning to search over potentially unstructured information with continuous, gradient-based learning methods. While the candidate-centric models can overcome this issue on the current set of machine comprehension tasks, they would not work in a more general QA setting where the correct answer does appear directly in the context. 

Since the work of~\newcite{kadlec2016text}, various contributions have incremented the state-of-the-art on both the CBT and Deepmind News datasets. Some of these approaches derive improvements from richer representation of the context:  \cite{kobayashi2016dynamic,weissenborn2016separating,sordoni2016iterative}. Others experiment with more sophisticated ways to access the context, including hierarchical attention~\cite{cui2016attention}, repeated memory access~\cite{cui2016attention,weissenborn2016separating,dhingra2016gated,sordoni2016iterative} and explicit entailment-style reasoning operations~\cite{trischler2016natural}. Following the success of the attention sum reader~\newcite{kadlec2016text},  all of the aforementioned models are candidate-centric rather than predicting answers from their full vocabulary. At the time of writing, the best performing model on the CBT is that of~\cite{sordoni2016iterative}, which combines a bi-RNN context representations with iterative memory access. The current best on the Deepmind News task is achieved by~\newcite{dhingra2016gated}, whose model uses a multiplicative operation to match query representations with increasingly high-level views of the context.\footnote{An excellent repository for many of the datasets, papers and results discussed in this section can be found at \url{http://uclmr.github.io/ai4exams/eqa.html}}

While the greater architectural sophistication of these latter models yields small improvements on the Deepmind News and CBT tasks, it may be that the more sophisticated datasets are needed to see the advantages of these reasoning components. As highlighted by~\newcite{chen2016thorough}, these tasks can be susceptible to simple solutions that do not seem to rely on principled semantic inference or reasoning. As such, the SQuAD and Who said What datasets, which are just starting to be tackled by the research community, may play an important part in guiding the development of models capable of increasingly human-like text understanding.

\section{Entailment}

\section{Dialogue and interactive QA systems}

Dialogue systems aim to model human discourse or conversation in a realistic way while interacting with human users. The applications of dialogue systems range from more the conversational, such as chatbots and online agents,, to the more goal driven, such as a telephone restaurant or hotel reservation systems. Research into dialogue systems extends back to the earliest days of natural langage processing, with systems such as ELIZA \cite{weizenbaum1966eliza}, whose rule-based responses gave a surprising sense of genuine understanding.     

The age of deep learning has not left dialogue research untouched. Recent approaches to dialogue involving deep neural networks can be roughly divided into two camps: those with the explicit objective of yielding applicable systems with clear and specific communication objectives (such as completing a restaurant booking), and those whose primary aim is to converse with users in a natural, human-like way, but whose function may be less well-defined. Beyond these two areas of core dialogue research, we also review a third body of research: \emph{interactive QA systems}. These systems align with goal-driven dialogue systems in modeling interactions with a clear function (eliciting or providing a fact or information). However, like conversational agents they aim to cover wide or even unrestricted semantic domains.     

\subsection{Goal-driven dialogue systems}
Research into goal driven dialogue systems has typically avoided placing all of the burden of representation learning onto their component neural networks~\cite{henderson2014word,mrkvsic2015multi,wen2015stochastic,wen2015semantically}. Instead, these approaches exploit a hand-coded symbolic representation of their internal semantic state. Because these models are applied to comparatively narrow functional domains (such as IT support, restaurant reservation or hotel booking), it is possible for humans to encode all possible dialogue states, by means of an ontology of allowable concepts, predicates and values. For instance, in the Cambridge Restaurant System~\cite{wen2016network} there are 99 possible restaurants, each characterised by six predicates (\emph{food, pricerange, area, address, phone, postcode}) with constrained sets of possible values. 

Once such an ontology has been defined, there are two components of the dialogue problem to which neural networks can be applied in such systems. The first is \emph{state tracking}; the task of updating the current semantic state of the system based on some previous state and some incoming language. The second is \emph{response generation}; the task of producing language that is useful to the user given the current state.~\footnote{We assume that systems process and produce written language in this review, but one or both of these components can be extended to recei
To overcome some of the challenges facing the more modular, goal-dve input or produce output as raw speech.}

\newcite{henderson2014word} propose employing separate RNNs for each possible predicate in the dialogue state ontology. As the dialogue proceeds, each RNN updates the likelihood of its predicate taking a particular value based on its previous internal state and new linguistic input (in the form of ranked hypotheses from an ASR system). While the model is trained on a very small amount of data (1612 short dialogues), the low dimensional output space for each RNN (the possible values for each dialogue state predicate) and an autoencoding initialisation procedure yield stronger performance than previous approaches to the same task based on bayesian generative graphical models. \newcite{mrkvsic2015multi} builds on this approach, by introducing a single network that predicts the values of all predicates in the dialogue state. It is thus able to model the likely interactions between concept features, such as the address of a restaurant and its price range. However, \newcite{mrkvsic2015multi} show that initialising multiple RNNs in this holistic way before later specialising them to predict the value of a particular predicate yields better performance than training a single generalist network. A similar technique is also applied to train a network that is able to generalise effective across multiple semantic domains (and hence state representations and ontologies). \newcite{mrkvsic2016neural} later show that feed-forward networks with access to only one prior time-step in the chain of dialogue interactions can be equally or more effective than RNNs for dialogue state tracking. 

Goal-driven dialogue systems have also benefited from the application of neural networks to the task of mapping from dialogue state representations to plausible linguistic output. \newcite{wen2015stochastic,wen2015semantically} apply RNNs to the task of natural language generation conditioned on dialogue states. This application mirrors the RNN decoding in the NMT systems described in section (X), which also produced striking results in image caption generation [REFs]. Generation for goal-driven dialogue systems differs in two key aspects from these applications: models typically have access to much less training data, and must be conditioned on symbolic rather than distributed semantic representations. \newcite{wen2015stochastic} overcome the former challenge by keeping network size very small (80 units in the hidden layer), making units dropout [REF] with a high (50\%) probability, and using pre-trained word embeddings in the output layer. \newcite{wen2015semantically} present an elegant solution to the latter obstacle by allowing a one-hot representation of the (symbolic) dialogue state to control a gate of an LSTM module in the generating RNN, which effectively moderates the flow of information from the symbolic representation to the word-by-word generation process. \newcite{wen2015stochastic} also observe improvements by re-ranking the top-n candidate sentences produced by their models. During training this is done by means of a backward RNN conditioned on future dialogue states. At test time, they use an (unconditional) convolutional neural language model fit to the utterances in the training data. 

A key challenge for goal-driven dialogue systems is obtaining training data, which must consist of conversations aligned with corresponding dialogue states. Even with creative crowdsourcing methods~\cite{wen2016network}, the sparsity of such data will be prohibitive for deep learning methods unless the number of possible dialogue states is small. While such ontologies can be defined for restricted domains such as the restaurants in a given city, it is less clear whether the approach is scalable, or whether hand-crafted semantic ontologies could be constructed for the full range of desirable dialogue domains and applications. 

\todo{Insert a table 1 from \cite{wen2015semantically} to show a typical ontology}

\subsection{Conversational dialogue models}
To overcome some of the challenges facing the more modular, goal-driven systems, these (give a general description of how these approaches work). 



\cite{shang2015neural} Multi-response for short text. RNN encoder decoder, and a combination of the two. Generates output different from NMT. Evaluate with human NOT BLEU which they argue is pointless and note it doesn't correlate with humans. Train on Weibo chinese corpus of responses on social media. multiple outputs per input. 

\cite{sordoni2015neural} twitter conversations. Feed forward bag-of-words encoders. Recurrent decoders. Best results separating the long-term background into a separate representation from the most recent response and concatenating. 

\cite{vinyals2015neural} classic encoder-decoder DONT CITE!

\cite{kalchbrenner2013recurrent} Dialogue act classification (cannot generate from this model, so maybe just mention out of politeness. 



\cite{serban2016building} First julian - movies, hierarchical. Hierarchical plus bidirectional. 

2) End-to-end / learning from raw data / latent semantics
- requires much bigger data
- acquire latent representations of state / topic / desire
- solution could scale to many domains provided data available but perhaps a long way off
- evaluation harder, can't ask whether the goal was achieved, since goal not even clear. 

\subsection{Interactive QA systems}

can mention DefGen here perhaps. Mostly Weston memory nets QA-style stuff. NTM? 

\subsection{Datasets}

\subsection{Discussion}

The problem of evaluation. Marginally easier for goal-driven systems. 

The spectrum of structured to open, the NN philosophy etc. 
ultimately gunning at the same thing: in 1) they form a narrow symbolic channel with which to reason and track behaviour. modular. but in that case the NN is just a smooth function approximator which happens to yield a satisfactory smoothness. Not solving the task itself. 
- a typical in-domain semantic state is very smal - 64 acts? 


\cite{serban2015hierarchical}
\cite{serban2016multiresolution}
\cite{serban2016hierarchical}
\cite{li2015diversity}
\cite{li2016persona}
\cite{li2016deep} %RL + dialogue
\cite{vinyals2015neural} %crap - only conversation

\cite{su2016line}




\cite{liu2016not}
\cite{su2015learning}


%datasets
\cite{lowe2015ubuntu}
\cite{serban2015survey}

\section{Representation Learning}
\subsection{Parsing}
\subsection{Distributed representations}

\section{Summarisation}

\section{Active Learning}

\section{Miscellaneous Others}










\bibliography{JAIR}
\bibliographystyle{JAIR}

\end{document}
