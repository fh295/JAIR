%
% File emnlp2016.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{JAIR}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{todo}

% Uncomment this line for the final submission:
\emnlpfinalcopy

%  Enter the EMNLP Paper ID here:
%\def\emnlppaperid{***}

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Natural Language Processing in the Age of Deep Learning: A Review}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
% \author{Felix Hill \and Daniele Pighin\\
%  {\tt publication@emnlp2016.net}}

\date{}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}


\section{Introduction}

In recent years, the use of artificial neural networks - a practice now generally referred to as \emph{deep learning} - has become increasingly popular in the field of natural language processing (NLP). Artificial neural networks have existed in various guises since at least the 1940s [REFs]. Indeed, some of their earliest applications were as models of human word learning [REF]. Nevertheless, the knowledge, software and hardware required to train such networks on the sort of large-scale datasets available to researchers in language technology have only recently become widespread. Since then, however, their proliferation has been striking. Deep learning approaches have now been successfully applied to numerous NLP sub-fields and tasks, from speech recognition and machine translation to question-answering and dialogue, in many cases resulting in at or near state-of-the-art performance.  

This paper is a review of the use of deep learning in NLP. It is intended for readers who would like an introductory overview of the important literature concerning the application of deep learning to key problems in language technology. While we provide some high-level description of the deep learning methods and techniques themselves, our principal focus is on \emph{the way in which they are applied, and their contribution to the different language problems}. As such, the sections correspond to the different domains and subfields of NLP. For finer details of learning algorithms and architectures, we instead refer the reader to the excellent explanations of [REF - DL textbook] and [REF - goldberg]. Similarly, we do not pretend to cover all literature connecting language and deep learning. Given the aforementioned proliferation of work, such an endeavour would necessitate very superficial treatment. Instead, we provide a deeper discussion and analysis of what we consider the most influential or important contributions in each area. 


\section{Background}

Small-scale language-related tasks going back to 1980s. Small-scale. Simulated language data / limited vocabulary. Nevertheless, distributed representations. 

Put content about DRs here. 

\paragraph{Input embeddings}
The first NNs applied to language did not compute an individual representation specific to each word. For instance, Elman's net X. In Hinton's simulations Y. 

However, the foundations for the current wave of were laid with Bengio language model. 


\section{Distributed representations of knowledge} The way in which the models considered in this thesis effect unsupervised learning is by acquiring \emph{distributed representations} of linguistic units (words, phrases, sentences and eventually short passages)~\citep{hinton1986learning}. Given a set \(S\) of \(s\) semantic (in this case, linguistic) concepts, we can encode each concept according to how strongly it activates each of a set of \(n\) potentially independent features. If each feature takes \(k\) possible values, we can unambiguously encode a maximum of \(k^n\) concepts in such a representation space. This contrasts with a situation in which the presence of each concept is represented by the activation of one of \(s\) discrete features. Such an encoding strategy is typically referred to as a \emph{symbolic} or \emph{localist representation}. The advantage of a symbolic approach is that, for a given view of the world (and thus a given set features and their activations) we can know immediately and unambiguously which concepts are present. On the other hand, in a system capable of representing \(n\) such features, a symbolic approach can only deal with \(n\) distinct concepts as possible inputs, whereas a system based on distributed representations could, in theory, reason about \(k^n\) concepts. 

There are additional reasons why systems built on distributed representations are popular when aiming to replicate core human capabilities such as object recognition or language understanding. First, distributed models naturally generalise because of shared attributes. While we may not be able to associate words or easily-delimited meaning with individual features in a distributed representation, we can observe that representations of related or similar concepts such as \(dog\) and \(wolf\) share common patterns of activation. A model that is trained exclusively on images of dogs (not wolves) eating meat may learn that concepts with a dog-like pattern of activation are carnivorous, from which it might infer that wolves are also carnivorous. Such generalisation is not possible using purely symbolic models, which do not naturally induce a similarity space on the concepts they encounter. It is, on the other hand, highly characteristic of human reasoning and cognition~\citep{rosch1976basic}. 

Second, a distributed model can acquire new concepts (beyond the original set \(S\)) without requiring additional memory, since concepts can `emerge' as (relatively) stable patterns of activation across its feature set. In the present example, the concept of \emph{carnivore} might be acquired if the model observes sufficient examples of animals eating meat (and different animals not eating meat). Over time, \emph{carnivore} would come to behave just like any other concept in the memory of the model (i.e. associated with a stable pattern of feature activations). This is not the case with symbolic models. While the attribute \emph{carnivore} could be associated with a subset of concepts in a symbolic model, it would not have the status of concept in itself unless the model added to its memory (and feature space). The existence of emergent concepts, of varying degree of stability or salience, is also a commonly-observed trait of human cognition~\citep{patterson2007you}. 

Third, like human semantic memory, knowledge encoded in distributed representations is content-addressable. If humans see a partially obscured object or hear an incomplete word, they may be able to infer the correct entity quickly based on this information. Similarly, if a distributed model receives partial information about a concept (in the form of an incomplete activation pattern), it may be able to access the full concept using this information (and indeed, may be able to learn to organise its memory such that its ability to perform such reconstructions improves). By contrast, in a symbolic model it is not generally possible to search for knowledge based on aspects of the information in question. Additional information about its location in memory is normally required. 

Finally, if we encode concepts via symbols, or even pre-specified semantic features (sub-concepts), we introduce a large degree of bias or ad hocness into our representations~\citep{miikkulainen1991natural}. The world is a large, chaotic continuum, and we have very little idea of how the brain rationalises the enormous stream of input data it receives. By training and then interrogating models that acquire representations of this data that are optimal for achieving realistic linguistic objectives, we can learn in a comparatively unbiased way which aspects of the data are important for realising the particular objective. This in turn can provides at least circumstantial evidence that the human brain exploits the same characteristics of the signal in order to achieve its learning goals.     

These natural effects of computing with distributed representations are particularly appropriate for modelling linguistic cognition. Semantic categories such as the referents of a word are notoriously hard to delimit in terms of defining properties~\citep{fauconnier1994mental}. English speakers can easily use and understand a word like \emph{chair}, and a seat with four legs and a back would typically be classed as a chair, but these properties are neither necessary nor sufficient for classifying an object as such. A longer seat with four legs and a back is not really a chair, but rather a \emph{bench}. Similarly, an armchair does not typically have four legs, and we might call a tree stump a chair if people sat on it regularly enough~\citep{prinz2004furnishing}. 

Western linguistic theory and education often involves more abstract categories referring to language itself, such as \emph{noun} or \emph{subject}, but even these can prove frustratingly intangible upon thorough consideration. Nouns are often said to refer to entities or objects, but \emph{failure} or \emph{unravelling} do not fall into either category. The type of word considered to be an \emph{verb} or an \emph{adjective} can be very different to speakers of different languages~\citep{anward1997parts}. How can we realise effective communication with others using and referring to concepts if we ourselves do not know how to unambiguously define or describe them? The solution advocated in this thesis is that we do so in the same way as a computational models that perform computations on distributed representations. This hypothesis also seems to be consistent with our (limited) knowledge the biological hardware of the brain~\citep{kiefer2012conceptual}.
 
\section{Deep learning} The terms \emph{artificial neural networks}, \emph{connectionism} and, now, \emph{deep learning} refer to a family of computational models that store knowledge in distributed representations and perform computations on these representations. In these approaches, distributed representations of the concepts observed in data are learned as part of the same process as learning how to interpret or manipulate these representations in some optimal way (e.g. to perform accurate prediction or classification based on the input). In many deep learning models, `manipulating' representations corresponds to computing additional layers of representation of the same input (via continuous but non-linear mathematical operations). In theory, these representations then encode increasingly abstract or sophisticated characteristics of the input concepts. For example, in a deep convolutional network trained to classify images of objects into (semantic) object categories, features in the initial representations correspond to crude visual properties such as the edges of objects, whereas higher-level representations (which are the ones on which serve as input to the output classifier) seem to encode properties that can be much more directly connected to the model's objective (such as the presence of eyes)~\citep{zeiler2014visualizing}. 

Every deep learning model acquires (at least one layer of) distributed representations for each concept that it observes. Unlike representations or encodings defined manually, the representations learned for a particular model are likely to be optimal or close to optimal with respect to objective of the model (given the constraints of its architecture). On the other hand, post-hoc analysis may be required to understand the representational strategy learned by the model, particularly since features in the model do not generally correspond to concepts that humans can label with words.        

\subsection{Neural language models} Deep learning models whose objective corresponds to linguistic tasks, such as predicting missing words, answering questions or classifying the sentiment of statements, are referred to as \emph{neural language models} (NLMs)~\citep{bengio2003neural}. The first NLM built on the scale of today's deep learning models was developed for the task of \emph{language modelling}; estimating the probability of a word sequence, or the related objective of predicting the next word in a text.\footnote{This should not discredit the various smaller-scale connectionist models of written language proposed before then (See e.g. ~\citealt{elman1990finding,miikkulainen1991natural})} In recent years, neural language models have begun to achieve state-of-the-art performance on a range of language processing tasks, including summarisation~\citep{rush2015neural}, visual question answering~\citep{antol2015vqa}, machine comprehension~\citep{hill2015goldilocks}, factoid question answering~\citep{bordes2014question}, entailment detection~\citep{rocktaschel2015reasoning} and machine translation~\citep{bahdanau2014neural}. In all of these cases, NLMs achieve at or close to state-of-the-art performance on the established benchmarks.  

Beyond this clear empirical case for focusing on NLMs (which was much less compelling at the beginning of my doctoral research) there are important scientific reasons for addressing language processing problems within the connectionist paradigm. Symbolic approaches that are able to learn and generalise about language typically rely on a pre-specified categorisation of linguistic input (for instance into classes like \emph{noun}, \emph{subject} or \emph{adjunct}). Such syntactic or semantic categories are not instantiated in the physical world, and in many cases there can be significant disagreement among language users regarding category membership~\citep{anward1997parts}. NLMs avoid this minefield by inducing such categories as (soft) clusterings of concepts in the similarity spaces of their distributed representations, inferring principles about these categories as computations on the spaces. By working with NLMs, we allow a (relatively) unbiased model to teach us more about the data, rather than relying on a human view of the data - and its inevitable social and cultural biases - to inform the design of the model. 



\newcite{bengio2003neural}
\newcite{miikkulainen1991natural}
\cite{miikkulainen1993subsymbolic}

\section{Language Modelling}

\section{Machine Translation}

\section{Machine Comprehension}

Following the development of recurrent neural networks capable of mapping from sequences to sequences and their successful application to machine translation, NLP researchers noticed that a similar approach could be effective in \emph{machine comprehension} style applications. The terms \emph{machine comprehension}, \emph{machine reading} typically refer to the broad class of task in which an algorithm has access to a passages of multiple sentences of prose (henceforth the \emph{context}) and a set of questions about the context of the passage (referred to here as \emph{queries}). The algorithm must select a correct (single or multi-word) answer to each question based on the information in the corresponding passage.  Machine comprehension have much in common with \emph{reading comprehension} questions that are used test human reasoning and/or linguistic proficiency in language learners. 

\subsection{Datasets and sub-tasks}
\paragraph{Deepmind News datasets} The current wave of neural network-based machine comprehension research was in large part triggered by the news comprehension dataset of~\newcite{hermann2015teaching}, compiled automatically from articles on the CNN and Daily Mail websites. News stories on both sites are summarised by a series of bullet points that summarise their main points. To develop their machine comprehension task,~\cite{hermann2015teaching} first identified all named entities in both the article and the bullet points using standard (NER) tools. A question in the CNN or Daily Mail dataset was then constructed by selecting an article, removing an entity from one of the bullet points in the summary and verifying that the entity does indeed appear in the article. To prevent the application of external knowledge to the task, the entities in the articles were replaced with symbolic identifiers. The task facing models is therefore to match the missing entity in each question to one of the symbolic identifiers identifiers in its article. The (smaller) CNN section of the Deepmind dataset alone consists of over 380,000 training and 4,000 test questions taked from over 90,000 distinct articles.

\paragraph{Facebook Children's Book Test (CBT)} The CBT is a multiple-choice test in which models must identify the missing word in a sentence from a children's book by using information from the previous 20 sentences in the book~\newcite{hill2015goldilocks}. The questions were selected such that both the missing word from the 21st sentence, and nine distractor candidate answers, appear at least once in the preceeding 20 sentences. The training and test questions are also (optionally) separated according to whether the missing word (and candidate choices) is a preposition, a verb, a common noun or a named entity. An important difference between the CBT and the Deepmind News dataset is that entities are not anonymised, so that general knowledge acquired from external resources can be used by models to improve performance on the test set. In total there are over 660,000 training and 10,000 test questions in the CBT, divided roughly equally according to the four question types. 

\paragraph{Stanford Question Answering (SQuAD) dataset} The SQuAD dataset,~\newcite{rajpurkar2016squad} crowdsourced English speakers to write questions that follow naturally from Wikipedia passages. Thus, models attempting the SQuAD task must answer authentic questions rather than cloze-style statements with missing words, bringing the task closer to genuine human reading comprehension. A further difference from the Deepmind and CBT datasets is that annotators were encouraged to write questions whose answers were multi-word segments in the article. For instance, the answer to the question \emph{what are two things that a computer always has} is the article segment \emph{a central processing unit (CPU) and some form of memory}. The SQuAD dataset consists of 90,000 training and 10,000 test questions. 

 \paragraph{TTI Who did What dataset} The Who did What dataset~\cite{onishi2016did} is based on articles in the English Gigaword Corpus. To compile the resource,~\newcite{onishi2016did} selected articles whose first sentence contained a person named entity. The person entity in question was removed from the sentence to form the question, the remainder of the article discarded, and a new (supporting) article retrieved that refers to the same person. To ensure they are sufficiently informative, the supporting articles were retrieved via an IE-style string matching procedure in which the question is used as the query. Questions were discarded if they could be answered by simple language model or frequency baselines, resulting in a bank of over 180,000 train and 10,000 test questions that humans can answer approximately 84\% of the time. As in the SQuAD dataset, answers in the Who did What dataset may consist of multiple words, although they are all person entities (\emph{the King of France}).

\paragraph{}Before the advent of neural machine comprehension models, research on machine comprehension typically relied on questions taken directly from real human reading comprehension questions~\cite{hirschman1999deep}. However, since such questions are usually produced by private education enterprises, large-scale datasets of this kind are not freely available for NLP research. Another notable forerunner to today's massive training and test resources is the \emph{MC Test}~\cite{richardson2013mctest}, which consists of 660 stories, and corresponding questions, all written by human annotators. The training data made available with the MC Test dataset is much smaller than more recent resources, and neural language models have not typically performed as well on this task as approaches that employ bespoke, hand-engineered symbolic features. However, recent work has shown that neural language models trained additionally on external data sources can perform competitively on the MC Test~\cite{trischler2016parallel}. Developing and understanding such transfer from general-purpose training data to specific small-scale applications is a key challenge for neural net approaches to machine comprehension and language understanding in general. 


\subsection{Models}
The first large-scale neural network approach to machine comprehension, the {\bf Deepmind recurrent readers}~\cite{hermann2015teaching}, exploits many of the techniques and principles of attention-based NMT (section XX). To answer a question from the Deepmind News dataset, the recurrent readers first apply a bidirectional RNN to the context article~\footnote{This encoding is similar to an attention-NMT model reading the source sentence, but in this case the bidirectional RNN covers multiple sentences}. This yields a variable-length distributed representation of the context containing a unique vector (the RNN hidden state) for each inter-word position in the article. 

In the \emph{attentive reader}, a single distributed representation of the query is computed using another bidirectional RNN, and used to compute attention weights over the set of context vectors. These are used to compute a single, fixed length (weighted sum) representation of the context, which is recombined with the query and fed to an output softmax optimised to predict the correct missing word from all possible words in the model's vocabulary.  In the \emph{impatient reader}, the attention weights, and hence a different weighted-sum representation of the article, is computed for each position in the bullet-point query. Thus, much like the attention-NMT model when processing a target sentence from the training set, the impatient reader can take a different perspective on the context depending on its current point of focus in the query. These perspectives are incremented to arrive at a (complex) single-vector representation of the article, based on which probabilities are computed for vocabulary words as potential answers (as in the attentive reader).

On the Deepmind News tasks, the recurrent reading models outperform a range of symbolic (non-neural) baselines, including those based on TFI-IDF and other techniques popular in information retrieval. They also outperform a neural baseline in which deep, bidirectional LSTM models read the query and the article in turn (as one long, conflated sequence) before predicting a possible answer.

This approach to large-scale machine comprehension was improved-upon by~\newcite{hill2015goldilocks} using a {\bf memory network}. Their model consists of four components. The first embeds the context in a finite set of distributed `memories'. The second weights these memories by matching them to a distributed representation of the query. The third computes a single representation of the memory as a weighted sum of the memories. The fourth predicts an output word via an output softmax over the model vocabulary. The network is trained end-to-end using a maximum likelihood objective. While a specific form of each of these components is also performed by the deepmind recurrent readers, the modular design of the memory network makes it possible to experiment with different strategies for each component.~\newcite{hill2015goldilocks} found particular methods for the first and second components that yielded improved performance on the Deepmind News dataset and strong benchmark performance on the CBT. The first component of the memory network that achieved this performance represented the context as a series of discrete 5-word windows.~\footnote{This representation probably allows the memory network to encode similar information to that encoded by a a bi-RNN at the point of reading the middle word in the window}. In the second component, it employed a `self-supervision' heuristic in which the weights in query and document representations were updated during training to directly increase the probability of retrieving the correct answer from the context relative to the other candidate answers. 

\newcite{kadlec2016text} continued the trend of improvement with the {\bf attention sum reader}, an elegant solution that combines effective aspects of both memory network and the Deepmind readers. Their approach is \emph{candidate-centric}: rather than estimating the probability of possible answers from a large vocabulary, it simply estimates the probability that some (or all) of the words or segments in the context is the correct answer. Consequently, it avoids the computation of an expensive and inefficient softmax over a large vocabulary, and can focus directly on locating a part of the context than indicates the correct answer. Specifically, the model reads the context using bidirectional LSTMs and stores representations focused on every candidate answer (the anonymous en will effectively preclude these simpler solutions andtities in the Deepmind News task or candidate answers in the CBT). A separate network represents the query (again with a bi-LSTM), and the candidates are scored based on their match with the query representation. The model is then simply trained to score correct answers above other candidates.  

The attention sum reader underlines the value of bidirectional recurrent nets in providing variable (`soft') windows of focus over unstructured text. Together with the self-supervision heuristic used by~\newcite{hill2015goldilocks} to train memory networks, it also highlights the importance (and the challenge) of learning to search over potentially unstructured information with continuous, gradient-based learning methods. While the candidate-centric models can overcome this issue on the current set of machine comprehension tasks, they would not work in a more general QA setting where the correct answer does appear directly in the context. 

Since the work of~\newcite{kadlec2016text}, various contributions have incremented the state-of-the-art on both the CBT and Deepmind News datasets. Some of these approaches derive improvements from richer representation of the context:  \cite{kobayashi2016dynamic,weissenborn2016separating,sordoni2016iterative}. Others experiment with more sophisticated ways to access the context, including hierarchical attention~\cite{cui2016attention}, repeated memory access~\cite{cui2016attention,weissenborn2016separating,dhingra2016gated,sordoni2016iterative} and explicit entailment-style reasoning operations~\cite{trischler2016natural}. Following the success of the attention sum reader~\newcite{kadlec2016text},  all of the aforementioned models are candidate-centric rather than predicting answers from their full vocabulary. At the time of writing, the best performing model on the CBT is that of~\cite{sordoni2016iterative}, which combines a bi-RNN context representations with iterative memory access. The current best on the Deepmind News task is achieved by~\newcite{dhingra2016gated}, whose model uses a multiplicative operation to match query representations with increasingly high-level views of the context.\footnote{An excellent repository for many of the datasets, papers and results discussed in this section can be found at \url{http://uclmr.github.io/ai4exams/eqa.html}}

While the greater architectural sophistication of these latter models yields small improvements on the Deepmind News and CBT tasks, it may be that the more sophisticated datasets are needed to see the advantages of these reasoning components. As highlighted by~\newcite{chen2016thorough}, these tasks can be susceptible to simple solutions that do not seem to rely on principled semantic inference or reasoning. As such, the SQuAD and Who said What datasets, which are just starting to be tackled by the research community, may play an important part in guiding the development of models capable of increasingly human-like text understanding.

\section{Entailment}

\section{Dialogue and interactive QA systems}

Dialogue systems aim to model human discourse or conversation in a realistic way while interacting with human users. The applications of dialogue systems range from more the conversational, such as chatbots and online agents,, to the more goal driven, such as a telephone restaurant or hotel reservation systems. Research into dialogue systems extends back to the earliest days of natural langage processing, with systems such as ELIZA \cite{weizenbaum1966eliza}, whose rule-based responses gave a surprising sense of genuine understanding.     

The age of deep learning has not left dialogue research untouched. Recent approaches to dialogue involving deep neural networks can be roughly divided into two camps: those with the explicit objective of yielding applicable systems with clear and specific communication objectives (such as completing a restaurant booking), and those whose primary aim is to converse with users in a natural, human-like way, but whose function may be less well-defined. Beyond these two areas of core dialogue research, we also review a third body of research: \emph{interactive QA systems}. These systems align with goal-driven dialogue systems in modeling interactions with a clear function (eliciting or providing a fact or information). However, like conversational agents they aim to cover wide or even unrestricted semantic domains.     

\subsection{Goal-driven dialogue systems}
Research into goal driven dialogue systems has typically avoided placing all of the burden of representation learning onto their component neural networks~\cite{henderson2014word,mrkvsic2015multi,wen2015stochastic,wen2015semantically}. Instead, these approaches exploit a hand-coded symbolic representation of their internal semantic state. Because these models are applied to comparatively narrow functional domains (such as IT support, restaurant reservation or hotel booking), it is possible for humans to encode all possible dialogue states, by means of an ontology of allowable concepts, predicates and values. For instance, in the Cambridge Restaurant System~\cite{wen2016network} there are 99 possible restaurants, each characterised by six predicates (\emph{food, pricerange, area, address, phone, postcode}) with constrained sets of possible values. 

Once such an ontology has been defined, there are two components of the dialogue problem to which neural networks can be applied in such systems. The first is \emph{state tracking}; the task of updating the current semantic state of the system based on some previous state and some incoming language. The second is \emph{response generation}; the task of producing language that is useful to the user given the current state.~\footnote{We assume that systems process and produce written language in this review, but one or both of these components can be extended to receive input or produce output as raw speech.}

\newcite{henderson2014word} propose employing separate RNNs for each possible predicate in the dialogue state ontology. As the dialogue proceeds, each RNN updates the likelihood of its predicate taking a particular value based on its previous internal state and new linguistic input (in the form of ranked hypotheses from an ASR system). While the model is trained on a very small amount of data (1612 short dialogues), the low dimensional output space for each RNN (the possible values for each dialogue state predicate) and an autoencoding initialisation procedure yield stronger performance than previous approaches to the same task based on bayesian generative graphical models. \newcite{mrkvsic2015multi} builds on this approach, by introducing a single network that predicts the values of all predicates in the dialogue state. It is thus able to model the likely interactions between concept features, such as the address of a restaurant and its price range. However, \newcite{mrkvsic2015multi} show that initialising multiple RNNs in this holistic way before later specialising them to predict the value of a particular predicate yields better performance than training a single generalist network. A similar technique is also applied to train a network that is able to generalise effective across multiple semantic domains (and hence state representations and ontologies). \newcite{mrkvsic2016neural} later show that feed-forward networks with access to only one prior time-step in the chain of dialogue interactions can be equally or more effective than RNNs for dialogue state tracking. 

Goal-driven dialogue systems have also benefited from the application of neural networks to the task of mapping from dialogue state representations to plausible linguistic output. \newcite{wen2015stochastic,wen2015semantically} apply RNNs to the task of natural language generation conditioned on dialogue states. This application mirrors the RNN decoding in the NMT systems described in section (X), which also produced striking results in image caption generation [REFs]. Generation for goal-driven dialogue systems differs in two key aspects from these applications: models typically have access to much less training data, and must be conditioned on symbolic rather than distributed semantic representations. \newcite{wen2015stochastic} overcome the former challenge by keeping network size very small (80 units in the hidden layer), making units dropout [REF] with a high (50\%) probability, and using pre-trained word embeddings in the output layer. \newcite{wen2015semantically} present an elegant solution to the latter obstacle by allowing a one-hot representation of the (symbolic) dialogue state to control a gate of an LSTM module in the generating RNN, which effectively moderates the flow of information from the symbolic representation to the word-by-word generation process. \newcite{wen2015stochastic} also observe improvements by re-ranking the top-n candidate sentences produced by their models. During training this is done by means of a backward RNN conditioned on future dialogue states. At test time, they use an (unconditional) convolutional neural language model fit to the utterances in the training data. 

A key challenge for goal-driven dialogue systems is obtaining training data, which must consist of conversations aligned with corresponding dialogue states. Even with creative crowdsourcing methods~\cite{wen2016network}, the sparsity of such data will be prohibitive for deep learning methods unless the number of possible dialogue states is small. While such ontologies can be defined for restricted domains such as the restaurants in a given city, it is less clear whether the approach is scalable, or whether hand-crafted semantic ontologies could be constructed for the full range of desirable dialogue domains and applications. 

\todo{Insert a table 1 from \cite{wen2015semantically} to show a typical ontology}

\subsection{Conversational dialogue models}
The previous section demonstrates that neural networks, and RNNs in particular, can be useful components of traditional dialogue systems. However, the promise of deep learning is that, eventually, internal representations like the semantic `state' of a dialogue system can be inferred automatically from data. Such a step would  obviate the need for expensive, domain-specific ontology engineering. Nonetheless, we are some way from training dialogue systems that are both capable of learning their internal representations and are of practical use to users with a concrete objective. Attempts to meet this challenge constitute a vibrant area of current research. The interface between semantics, language learning and dialogue promises to be one of the most interesting applications of deep learning and language technology in the coming years. 

As with a growing number of NLP application areas, an important basic element of many data-driven, conversational dialogue models is the sequence-to-sequence neural network originally applied to speech recognition and MT. Given a (sufficiently large) corpus transcribing an ordered set of discrete utterances \(u_1 \dots u_n\), perhaps coming from different speakers, a sequence to sequence architecture can model the function mapping one utterance to the next utterance simply by treating the data \(\{u_1, u_2\}\), \(\{u_2, u_3\}\) etc. as \{source, target\} pairs. Once trained in this way, a sequence-to-sequence model given an utterance \(u\) can generate a response utterance \(v\) by approximating the highest probability sentence from the output distribution, much as with NMT models. As shown by both~\cite{sordoni2015neural} and~cite{vinyals2015neural}, models trained in this way capture aspects of typical conversational flow in a compelling way. 

However, there are fundamental limitations to modelling dialogue with a (deterministic) function in this way. Unlike in MT, the mapping between conversational utterances and their responses is evidently not one-to-one - a wide range of possible utterances can be met with the response \emph{I don't know}. Further, to an even greater extent than in MT, in human dialogue there is no single `correct' response to any particular utterance. Motivated by these observations,~\cite{shang2015neural} train a sequence-to-sequence model on a multi-response corpus consisting of social media (Weibo) posts together with set of \(n\) resulting comments, where typically \( 1 \leq n \leq 10\). In other words, the model is trained explicitly to approximate a one-to-many function. Comparing models based on human judgements of their grammaticality, consistency, informativeness etc,~\cite{shang2015neural} find that sequence-to-sequence models cope well in this training regime, outperform a symbolic statistical MT-style model, and that sequence-to-sequence models with attention (Section XX) outperform those without attention. In a related contribution, ~\newcite{li2015diversity} observe an undesirable characteristic of sequence-to-sequence models trained on dialogue corpora. The prior probability of generic or uninformative responses such as \emph{I don't know} in such a corpus is always comparatively high, since such responses can appropriately follow almost any utterance. Therefore, at test time, dialogue sequence-to-sequence models trained, like the best-perfoming NMT mdoels, to maximise the likelihood attributed to the training data produce uninformative responses with high regularity. ~\newcite{li2015diversity} propose a way to mitigate this by down-weighting the importance of this prior probability in the training data. They find that humans judge the output of models to be more interesting and informative as a consequence of being trained in this way.~footnote{See BENGIO and RANZATO for other creative ways of improving the output of sequence prediction models}. 

Another challenge in modelling dialogue is the long-term interactions between utterances. A typical human conversation is full of references to entities and facts mentioned previously in the same conversation. However, RNNs trained on human dialogue data generate utterances that clearly contradict those made several turns earlier in the dialogue, yielding conspicuously non-human output [INSERT EXAMPLE]. Moreover, using LSTMs or GRUs to augment the sensitivity of RNNs to longer temporal dependencies does not effectively resolve the issue.~\newcite{sordoni2015neural} instead employ a feedforward (bag-of-words) architecture to encode all previous dialogue utterances in a single vector, which is concatenated to a representation of the latest utterance (effectively giving equal status to the current utterance and all previous utterances combined). This combined representation is then mapped to a suitable response using a standard RNN decoder. \newcite{sordoni2015neural} finds that the addition of this longer-term perspective improves performance compared with a model that responds based on the most recent utterance alone. \newcite{serban2016building} extend this idea with a two-level RNN architecture involving three components. The lower RNN encodes utterances much as in a standard sequence-to-sequence model. A dialogue tracking RNN reads the final hidden state of this encoder (a distributed representation summarising its content), updates its own internal state and produces some output representation. A decoder RNN then predicts a suitable (linguistic) response given this output of the state tracking RNN. This approach is more expressive than that of \cite{sordoni2015neural} in two important ways. First, because it reads all linguistic input with RNNs it can encode order-dependent aspects of all utterances. Second, the state-tracking RNN allows the model to learn (and generalise over) common sequential dependencies in high-level semantic flow of the conversation.\footnote{In this sense, the highest level network in~\cite{serban2016building} is analogous to the specialised state-tracking RNNs proposed by~\newcite{mrkvsic2015multi}}. Importantly, while the two-level model involves three RNNs, it can be trained fully end-to-end like any other sequence-to-sequence model, so that the component networks are optimised jointly to increase the likelihood of modelling observed conversation data. Thanks to its greater expressivity, however, the two-level RNN exhibits lower perplexity on held-out conversation data than both the standard sequence-to-sequence approach and the (flatter) contextual model of~\cite{sordoni2015neural}.

~\cite{li2016persona} tackle an orthogonal problem of giving each interlocutor as a distinct persona, by introducing additional latent representations (embeddings) for storing information associated each of the contributors in a conversation. The a given speaker's response can then be conditioned on both on the previous utterances and the appropriate persona embedding, which yields greater within-person consistency in the model output. For instance, the model output for Person 1 might claim to be British and later to like fish and chips, whereas the responses of Person 2 indicate that she is Italian and, later, that she likes pizza.

Finally, [REF] experiment with ways of promoting a richer form in the internal semantic states of their dialogue model, but still within a fully data-driven framework. Their technique employs the same three recurrent networks as the two-level deterministic architecture described by \newcite{serban2016building}. However, in the latent-variable architecture, the lower RNN and dialogue-tracking RNNs together map utterances in context to appropriate values for the mean and covariance of a series of multivariate Gaussian latent variables (one for each turn in the dialogue). To generate a response at test time for turn \(t\) in context \(c\), a sample vector of real values is drawn from the appropriate (multivariate) latent variable, and combined with a separate encoding of \(c\) to yield the initial state of the decoder RNN, which can then predict a response word-by-word as before. The parameters of the encoder, dialogue-state and decoder RNNs are trained via back-propagation using a technique first applied to \emph{variational auto-encoders}~\cite{kingma2013auto} such that they both model the flow of the dialogue and predict appropriate values for the stochastic latent variables. Compared with the fully-deterministic hierarchical model,~\newcite{serban2016hierarchical} show that the latent variable model yields responses that are rates as more satisfactory by human annotators. Further, the stochasticity in the latent-variable approach aligns with the fact that a given dialogue turn may be consistent with multiple correct responses. Finally, such approaches may ultimately yield data-driven models with more structured and interpretable internal semantics, as, qualitatively, particular dimensions in the latent space can often correspond to identifiable linguistic or semantic effects. 

\subsection{Simulated data approaches}

Surveying the landscape of dialogue research, it is clear that both goal-directed and end-to-end data-driven systems suffer from important limitations that could prevent each respective approach from yielding widely-adopted, general purpose dialogue systems. This has led to a strand of recent research in which end-to-end neural network agents (with no task-specific hand engineering) are trained on pseudo-natural, but algorithmically generated, language data designed to contain certain information and/or exemplify particular reasoning patterns. This approach can be considered as a `hybrid' between goal-driven and fully data-driven approaches. Hybrid approaches based on simulated data aim to couple the scalability, domain-adaptation, cost and naturalness advantages of end-to-end approaches with many of the benefits of goal-driven systems, such as direct functional utility and clear performance measurement.

An important motivating insight for the hybrid approach is that there are distinct types of abstract reasoning required by dialogue agents to arrive at useful output, which can be exemplified by distinct `tasks' (the \emph{BABI Tasks}) represented by canonical or prototypical dialogue exemplars. Unlike the symbolic semantic states specified by the developers of goal-driven dialogue systems, these tasks reflect abstract reasoning patterns and are therefore pertinent to dialogue from any linguistic or semantic domain. For example, the `two supporting facts' task exemplifies reasoning that straightforwardly combines information from two previous utterances in some dialogue in order to produce a sensible response. The `simple negation' class explicitly requires understanding that propositions in some dialogue are either negated or not in order to arrive at a satisfactory response.~\newcite{weston2015towards} define ten classes of reasoning intended to cover many of the common types of inductive step encountered in dialogues between users and agents and provide thousands of algorithmically-generated exemplars for each class (SEE TABLE X). If an architecture trained on these exemplars performs well on the corresponding test set we can conclude that the architecture is capable of learning the reasoning pattern in question. Using this approach, ~\newcite{weston2015towards} show that Memory Networks (SEE SECTION X) are capable of learning  ZZZ, but struggle with XX. This is better than ZZZ. 

The simulated data approach is more labour-intensive that fully data-driven dialogue systems trained on naturally-occurring transcripts. Human `experts' are required both to design the taxonomy of reasoning tasks (which by its nature is a somewhat ad-hoc process) and to produce the algorithms that turn such abstract tasks into real language (at present, using manually-specified vocabularies and syntax). However, unlike fully data-driven methods, they explicitly probe the ability of models to learn and execute formal reasoning that seems to be an inherent aspect of human dialogue and consequently a useful dialogue agent.\footnote{This is particularly important in the context of neural language models, given historical scepticism about their ability to efficiently execute formal (mathematical-style) reasoning REF.} In addition, they enable a more precise understanding of what different models and architectures are capable of learning and, consequently, where future research effort should be focused.

While the BaBI Tasks focus on the logical steps needed to execute conversations, they do not test an equally important function of practical dialogue systems - the ability to retrieve and apply facts (from potentially large knowledge sources) to serve user needs. Nevertheless, the `hybrid' approach based on simulated language can also be applied to develop and test systems with this focus. The basic idea is, given a structured database of facts, to algorithmically generate phrases or sentences that convey all of the information in the database. Neural language models can then be trained end-to-end on this large set of training examples. The trained model should in principle then be able to manage factual information similarly to a goal-driven dialogue system, but without the need for an intermediate symbolic semantic representation sitting between inputs and outputs and requiring expert engineering. Moreover, since all knowledge is stored in distributed memory, the system should (in theory) encode knowledge more efficiently, and permit richer generalisation over entities, properties and relations, than methods that keep the information in symbolic form.~\cite{dodge2015evaluating} apply this approach to large databases of movie information, creating training and test sets that couple questions (\emph{Can you name a film directed by Stuart Ortiz?} with answers (\emph{Grave Encounters}) both generated automatically (using templates) from the underlying database. Even though the questions in the test set are not answered in the training set, the entities in the test questions do appear in training question. Consequently neural language models are able to learn from the correspondence between QA pairs in the training set (i.e. patterns of inference) to perform well at answering the test questions.~\cite{dodge2015evaluating} show that the best performing models on such datasets are Memory Networks whose finite-size long-term memory is filled using a symbolic (IR-style) entity-matching operation over the entire test set prior to answering each question. ~\cite{bordes2016learning} go a stage further in bridging the gap between end-to-end and goal-driven dialogue systems by applying the simulated-data approach to the same restaurant-recommendation task addressed by the dialogue state-based goal-driven models. Their promising results using memory networks and other end-to-end models on this data suggests that ultimately it may be possible to learn functional dialogue states directly from data. Nevertheless, at present the work required specify the generation of simulated data is arguably as laborious as defining the symbolic framework of allowable dialogue-states in goal-driven systems. It is user satisfaction with the trained dialogue agent that should ultimately decide which of the two approaches is preferable.

\subsection{Datasets}
Any recording or transcript of conversation can potentially be a useful training resource for dialogue systems. Consequently, there is a great variety of potentially relevant corpora, which are surveyed in detail by~\cite{serban2015survey}. We therefore outline only those that have proved useful to date in training the more recent neural network-based dialogue models.

The Dialogue State Tracking Challenge (DSTC) corpora have proved to be an important resource and testbed for goal-driven dialogue models, such as the RNN-based state trackers of~\newcite{henderson2014word} and \newcite{mrkvsic2016neural}. Each of these corpora contains between 3,000 and 15,000 dialogues recording interactions between humans and rudimentary dialogue systems, and each is focused on a different practical domain. For instance, DSTC 1 transcribes interactions with an automatic bus ride system, and DSTC 3 records interactions with a restaurant booking system.~\cite{wen2016network} describe how such data can be collected using crowd-sourcing by following a \emph{Wizard-of-Oz} approach in which half of the recruited annotators operate in the role of 'human` (with an assigned goal to find certain information) and the other half in the role of machine (wizards - who record the information obtained to date and respond appropriately). Note that this mode of data collection relies on some form of pre-specified semantic dialogue state representation, which is needed for the wizards to track knowledge in a consistent way. 

End-to-end, data-driven, conversational dialogue systems, which do not rely on hand-engineered semantic representations, typically require much more raw data to train. Research on such systems has therefore tended to exploit larger datasets that were not originally constructed specifically for training dialogue systems, such records of social media posts, tweets or comments. Work that uses such resources include the encoder-decoder model of~\cite{shang2015neural}, whose Sina Weibo Corpus~\cite{shang2015neural} records multiple responses (from different users) to each of 4.5m initial postings.~\newcite{sordoni2015neural},~\newcite{li2015diversity},~\newcite{serban2016multiresolution} all experiment with similar datasets containing over 100 million context-message-response or A-B-A triples extracted from Twitter. Despite the enviable size and naturalness of these social-media resources, however, they cannot be shared publicly because of privacy and/or copyright restrictions. The application of similar datasets should therefore be constrained to bootstrapping model training, as comparing different models on such datasets are not replicable.

The Ubuntu Corpus~\cite{lowe2015ubuntu} is a notable, recently-released large-scale resource that does not suffer from the same legal restrictions and appears ideally suited for training dialogue-systems from raw-data in an end-to-end way. It consists of almost 1 million multi-turn, two-person dialogues collected from the logs of a popular chat application in which users seek and receive help with the Ubuntu operating system. 98\% of the dataset is intended for training and simply transcribes such interactions. 2\% of the data is allocated for evaluation, and for each dialogue a specific \emph{context} (several consecutive utterances), correct \emph{response} (the following utterance) and incorrect response (an utterance sampled randomly from elsewhere in the test-set) are combined to create a binary-choice question. Models are then scored for the proportion of questions in the test set (above the random baseline of 50\%) for which they can identify the correct response.

\subsection{Evaluation methods}

Evaluating and comparing dialogue systems and models is notoriously challenging and problematic. Goal-driven dialogue systems typically involve disjoint modules for dialogue-state tracking and response generation. Given a resource such as the DSTC corpora, evaluation of dialogue state tracking is comparatively straightforward: a portion of data is held-out for testing and a given tracking module can be scored on this test set for its ability to predict the appropriate state given the dialogue in question~\cite{henderson2014word,mrkvsic2016neural}. However, this method of evaluation is relatively laborious (and costly) since it requires humans both for defining the dialogue-state frameworks and for achieving the aligned conversation data. A further, and perhaps more fundamental, limitation is that such it can only be applied to models that explicitly abide by the representational framework defined by the human architect. If deep-learning is to play any role in a significant advance in dialogue systems, however, it will surely be by exploiting the capacity of neural networks to learn bespoke representations, potentially online, from raw conversation and dialogue data.    

However, the additional flexibility and generality of end-to-end, data-driven approaches pose their own problems when it comes to robust evaluation.~\newcite{sordoni2015neural} compare various end-to-end models based on average per-word perplexity on some held-out data similar to the training set. Using perplexity as an evaluation metric for dialogue effectively reduces dialogue to language modelling, and has several limitations. First, it does not measure the capacity of the system to generate an answer given some input, but merely to quantify the relative quality of possible responses. Second, a model at a given point in dialogue is not rewarded for exhibiting low perplexity for any responses beyond the single response in the test data, even though in practice numerous responses may be viable, particularly since no explicit goal is defined for the dialogue.  A promising way to avoid some of these problems is exhibited in resources such as the Ubuntu Corpus~\cite{lowe2015ubuntu}, which requires models to select a valid response from multiple, potentially incorrect candidates, mitigate many of these limitations. Indeed, the simulated-language datasets used by hybrid approaches~\cite{weston2015towards,bordes2016learning,dodge2015evaluating} also fall into this category, since models are compared on how highly the rank the correct (or recommended) entity among all possible entities in the ontology, vocabulary or knowledge base. 

Nevertheless, a model capable of consistently distinguishing good and bad responses is not necessarily capable of generating useful and/or interesting dialogue responses. To factor in all facets of a dialogue model into a single evaluation, one option is to directly score the quality of language generation. For neural language models, one popular approach is to actively generate from the models (by approximating MAP estimates from the output distribution with beam search, as in MT systems). The contents these ranked lists can be compared with the correct test response using metrics such as BLEU or METEOR~\cite{li2015diversity,li2016persona}. However, such evaluations suffer particularly from the second limitation, exacerbated by the fact that BLEU-style metrics are based simply on word or n-gram overlap. This means that even if a model response matches is grammatical and means the same as with the test response, the model will not be credited unless precisely the same words are used. Perhaps unsurprisingly, then, it has been observed that BLEU-style evaluations exhibit negligible correlation with human ratings of system quality~\cite{liu2016not}. Attempts to mitigate this critical flaw by using pre-trained word embeddings to generalise output and test responses have so far had mixed success~\cite{liu2016not}.

An obvious alternative, in the case of both end-to-end and goal-driven dialogue systems, is to conduct evaluations by recruiting human users to rate responses. Thanks to crowd-sourcing, this method of evaluation is increasingly popular~\cite{serban2016hierarchical,wen2016network}, and promises to overcome many of the drawbacks associated with automatic evaluations noted above. However, beyond the obvious drawbacks of cost and effort, human evaluations of dialogue systems are hard to design and formalise. First, it is unclear what exactly about a dialogue experience a human user should be asked to rate (i.e. content, style or both). Second, there is inherent subjectivity and inter-sample variability in human evaluation that can make it difficult to reach meta-conclusions from comparisons across different studies. 

Based on all of these considerations, the optimal way of evaluating dialogue systems at present - whether goal-driven or data-centric or somewhere in between - seems to be a  combination of both large-scale answer selection tasks (as in the Ubuntu Corpus) and carefully designed human evaluation.  


\section{Representation Learning}
\subsection{Parsing}
\subsection{Distributed representations}

\section{Summarisation}

\section{Active Learning}

\cite{weston2016dialog} Simulated conversation in which questions are asked at the end, and the goal is to see how well agents are at learning from the dialogue, given different reward and cost function setups. 

Angeliki paper

\cite{li2016deep} %RL + dialogue

\section{Miscellaneous Others}

Note image captioning is covered in a JAIR review - Bernadi et al. 








\bibliography{JAIR}
\bibliographystyle{JAIR}

\end{document}
